{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## HELPER FUNCTIONS AND MODULES\n",
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sigmoid( z ):\n",
    "    \"\"\" This shrinks our beta estimate into logistic form \"\"\"\n",
    "    return 1 / ( 1 + np.exp( -z ) )\n",
    "\n",
    "def cost_ll( X, y, betas ):\n",
    "    \"\"\" \"\"\"\n",
    "    # this is the current estimate\n",
    "    p = sigmoid( np.dot( betas, X.T ) )\n",
    "    # this measures the difference between the guess and the actual\n",
    "    return -np.sum( y*np.log( p+1e-8 ) + (1-y)*np.log( 1 - p+1e-8  ) ) \n",
    "\n",
    "def gradient( X, y, betas ):\n",
    "    \"\"\" This calculates the gradient given X, y and our betas \"\"\"\n",
    "    # when stochastic gradient descent is computed over a \n",
    "    # vector of X and a vector of betas with the y response\n",
    "    #  - need a part to check for a vector\n",
    "    return np.dot( ( y - sigmoid( np.dot( X, betas ) ) ), X )\n",
    "\n",
    "def approx_hes( X, y, betas ):\n",
    "    \"\"\"\n",
    "    Newtons method creates a weight matrix based on the 2nd derivative of each beta \n",
    "    \"\"\"\n",
    "    # needed to initialize a matrix approximating the hessian\n",
    "    N, P = X.shape\n",
    "\n",
    "    # logistic estimate\n",
    "    z    = sigmoid( np.dot( X, betas ) ) \n",
    "\n",
    "    # function as weight or coefficents on X\n",
    "    W    = np.multiply( z, ( 1.0-z ) ) \n",
    "\n",
    "    # coefficients for our approximate hessian \n",
    "    ap_H = np.zeros( (P, P) )\n",
    "\n",
    "    # cycles through each observation (N) adding the outer product of the X values\n",
    "    # and multiply that by the W vector used in the regular hessian calculations. \n",
    "    for obv in np.arange( N ):\n",
    "        ap_H += np.multiply( W[obv], np.outer( X[obv], X[obv] ) )\n",
    "    return ap_H\n",
    "\n",
    "def cost_ll( X, y, betas ):\n",
    "    \"\"\" \"\"\"\n",
    "    # this is the current estimate\n",
    "    p = sigmoid( np.dot( betas, X.T ) )\n",
    "    # this measures the difference between the guess and the actual\n",
    "    return -np.sum( y*np.log( p+1e-8 ) + (1-y)*np.log( 1 - p+1e-8 ) ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Search\n",
    "\n",
    "### Choosing the best line length\n",
    "\n",
    "- We can instead decide how large of a step we take during stochastic gradient descent.\n",
    "\n",
    "- Below is a line search that uses the wolfe conditionts to compute an alpha.\n",
    "\n",
    "- In order to do this we need a few functions. See Nocedal & Wright for details (expand to specific areas to reference) - much of the notation used in the code follows notation used in Nocedal & Wright.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls( betas, p, f, gf, c=1e-8, iterations=100, s=0.75):\n",
    "    '''\n",
    "     This function uses the wolf conditions to compute the \"optimal\" stepsize for \n",
    "     the next step in a descent method.\n",
    "\n",
    "    '''\n",
    "    N, P = X.shape\n",
    "    \n",
    "    # the loss function\n",
    "    fx = f( betas )\n",
    "    \n",
    "    # the gradient function multipled by the direction vector\n",
    "    gfp = gf( betas ).dot( p )\n",
    "    \n",
    "    # initialized alpha\n",
    "    a = 1.\n",
    "    \n",
    "    # because it's not working...\n",
    "#     print( \"p =\", p )\n",
    "#     print( \"fx =\", fx )\n",
    "#     print( \"gfp =\", gfp ) \n",
    "    \n",
    "    # a boolean for knowing if we've found one - we haven't yet. \n",
    "    eee = False\n",
    "    \n",
    "    # what's the best length given p one? \n",
    "    for iters in np.arange( iterations ):\n",
    "        \n",
    "        # calculate the potential\n",
    "        fxap = f( betas + a * p )\n",
    "#         print( \"a =\", a )\n",
    "#         print( \"fxap =\", fxap )\n",
    "#         print( \"fx + c * a * gfp =\", fx + c * a * gfp )\n",
    "\n",
    "        # if the potential leads to smaller cost, then go for it. \n",
    "        if fxap <= fx + c * a * gfp:\n",
    "            eee = True\n",
    "            return eee, a\n",
    "        \n",
    "        # if not, then scale alpha down (by s) and retry!\n",
    "        else:\n",
    "            a*=s\n",
    "    \n",
    "    print(\"seems like there is no better alpha...\")        \n",
    "    return eee, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_gd( X, y, iterations=1000, alpha=0.01, tol=1e-8):\n",
    "    \"\"\" This function performs 'simple' gradient descent.\n",
    "        - X is the data matrix - X is scaled before performing descent\n",
    "        - y are the actual values \n",
    "        - beta vector is initialized at a vector of zeros \n",
    "        - alpha or the learning rate is by default 0.00001 \"\"\"\n",
    " \n",
    "    # initialize betas vector\n",
    "    betas = np.zeros( X.shape[1] )\n",
    "    \n",
    "    # initialize empty convergence vector and empty convergence difference vector\n",
    "    cvg = []\n",
    "    betas_hist = []\n",
    "    steps = []\n",
    "    for iters in np.arange( iterations ):\n",
    "        \n",
    "        # get initial cost\n",
    "        if iters == 0:\n",
    "            cvg.append( cost_ll( X, y, betas ) )\n",
    "            betas_hist.append( betas )\n",
    "            \n",
    "        # ammend to adjust alpha based on iteration step\n",
    "        betas += alpha * gradient( X, y, betas )\n",
    "        betas_hist.append( betas )\n",
    "        \n",
    "        # save log likelihood output for plotting\n",
    "        cvg.append( cost_ll( X, y, betas ) )\n",
    "        steps.append( alpha )\n",
    "        \n",
    "        # check for convergence \n",
    "        if np.abs( cvg[-1] - cvg[-2] ) > tol:\n",
    "            pass            \n",
    "        else:\n",
    "            # we made it! how many iterations?\n",
    "            print(\"iterations to converge, %d \"%iters )\n",
    "            return betas, betas_hist, cvg, steps\n",
    "        \n",
    "    print(\"Did not converge, lower tolerance?\")\n",
    "    return betas, betas_hist, cvg, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lsgd( X, y, tol=1e-4, iterations=1000, c_up=0.5, s_up=0.75 ):\n",
    "    '''\n",
    "    This function performs a line search gradient descent method to find the beta\n",
    "    that minimizes the negative log likelihood function.\n",
    "    \n",
    "    Inputs:\n",
    "    X: an m by n array of features\n",
    "    y: an m vector of successes\n",
    "    beta: an initial guess for the minimizer (for which we are solving)\n",
    "    m: an m vector of the number of trials\n",
    "    tol: a float, optional to specify the error size used to indicate convergence\n",
    "    iters; an integer, optional to specify the max number of iterations for the\n",
    "         algorithm\n",
    "    cval: A float, option to specify the constant value used to determine stepsize\n",
    "        in linesearch\n",
    "    print_total: A boolean to determine whether the number of iterations it takes \n",
    "        to converge should be printed or not.\n",
    "         \n",
    "    Returns:\n",
    "    beta0: the minimizer of the negative log-likelihood\n",
    "    betas: a list of the new beta for each time step\n",
    "    ll: a list of the log likelihoods for each time step\n",
    "    '''\n",
    "    N, P = X.shape\n",
    "    \n",
    "    # initialize betas vector\n",
    "    betas0 = np.zeros( P )\n",
    "    \n",
    "    def f(betas):\n",
    "        # formats function for ls function\n",
    "        return cost_ll(X, y, betas )\n",
    "    \n",
    "    def gf(betas):\n",
    "        # formats function for ls function\n",
    "        return gradient(X, y, betas )\n",
    "    \n",
    "    # keep track of some things.\n",
    "    cvg = [] \n",
    "    steps = []\n",
    "    betas_hist = []\n",
    "    for iters in np.arange( iterations ):\n",
    "        # before\n",
    "        cost_ll_betas0 = cost_ll( X, y, betas0 )\n",
    "        cvg.append( cost_ll_betas0 )\n",
    "        betas_hist.append(betas0)\n",
    "\n",
    "        # first get the descent direction\n",
    "        p  = -gradient( X, y, betas0 ) \n",
    "        eee, alpha = ls( betas0, p, f, gf, c=c_up, iterations=1000, s=s_up )\n",
    "        \n",
    "        # collect the step sizes\n",
    "        if eee is not True:\n",
    "            steps.append( alpha )\n",
    "            print( \"stepsize no bueno, no lines\" )\n",
    "            return betas, betas_hist, cvg, steps\n",
    "        \n",
    "        # if we found a good line then lets use it! (save alpha)\n",
    "        steps.append( alpha )\n",
    "        betas1 = betas0 + alpha * p\n",
    "        \n",
    "        # after\n",
    "        cost_ll_betas1 = cost_ll( X, y, betas1 )\n",
    "        cvg.append( cost_ll_betas0 )\n",
    "        \n",
    "        # checks for convergence\n",
    "        if np.abs( cost_ll_betas1 - cost_ll_betas0 ) > tol:\n",
    "            beta0 = beta1\n",
    "        else:\n",
    "            # we made it! how many iterations?\n",
    "            print(\"iterations to converge, %d \"%iters )\n",
    "            return betas1, betas_hist, cvg, steps\n",
    "        \n",
    "    return betas1, betas_hist, cvg, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST IT OUT ON SOME RANDOM DATA!\n",
    "N = 400\n",
    "P = 8\n",
    "\n",
    "# Pre-process, add intercept\n",
    "X = np.hstack( (np.ones( (N,1) ), np.random.rand( N, P) ) )\n",
    "y = (np.random.rand( N ) > .5).astype(float)\n",
    "\n",
    "print( X.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations to converge, 0 \n",
      "iterations to converge, 290 \n"
     ]
    }
   ],
   "source": [
    "# lgsd code not working\n",
    "\n",
    "betas, betas_hist, cvg, steps     = lsgd( X, y, tol=1e-8, iterations=1000, c_up=.5, s_up=.75 )\n",
    "betas1, betas_hist1, cvg1, steps1 = logit_gd( X, y, iterations=50000, alpha=0.001, tol=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.212670403551897\n",
      "14.490204362369457\n",
      "0.9572060029314421\n"
     ]
    }
   ],
   "source": [
    "# comparing the difference in estimates between the two...\n",
    "y1 = X.dot( betas.T )\n",
    "y2 = X.dot( betas1.T )\n",
    "print( la.norm(y1 - y) )\n",
    "print( la.norm(y2 - y) )\n",
    "print( la.norm(betas - betas1) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11ca31d30>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEKCAYAAADn+anLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKSRERCQohYSIiAT1/wFD5IpinYbZ6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11cdb4a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the convergence paths\n",
    "plt.plot(cvg, label=\"Line Search, wolfe-conditions\")\n",
    "plt.plot(cvg1, label=\"alpha=0.0001\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something is not happening correctly in my line search algorithm...\n",
    "\n",
    "The line search algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Comparing convergence for various values of C\n",
    "# strightforward if line search alogrithm is working...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quasi Newton Methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
