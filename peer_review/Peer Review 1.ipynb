{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *_Exercise 1: Peer review_*\n",
    "\n",
    "#### WLS\n",
    "\n",
    " - In the qr function, I wasn't sure why a weight of 0.5 was broadcasted to the rest of the X matrix. Although, again with using the broadcasting, I think it would be much quicker than diagonalizing and then multiplying. This also relates to some of the work done in the GLM code so I ran a little test between broadcasting and matrix multiplication with diagonal of weights. Broadcasting is obviously much quicker. I'm glad I came across this, certainly this would scale much better for larger datasets!\n",
    "\n",
    "\n",
    "#### GLM\n",
    "I'm going to first go through general comments about the exercises I reviewed and the go into detail. \n",
    "\n",
    "__Overall__\n",
    "\n",
    "It would have been nice to see a context for each of the problems that were going to be worked - much like how the exercise is presented in the document. This way each of the pieces of code were motivated. \n",
    "\n",
    "I did learn a number of things about jupyter notebook and python that I had not learned before. It was also nice to see the solutions to the problems formatted in quite a different way than I did for my solutions. Below are a few basic python and jupyter things I learned that were beneficial to me.\n",
    "\n",
    "__Pros__\n",
    "\n",
    " - I did not realize the entire document read imported packages, for whatever reason I thought it had to do with corresponding cells...\n",
    "\n",
    " - There was also a sophisticated way of using broadcasting to implement the hessian matrix that I thought was particularly enlightening. Something I could look to take advantage of in my own code. In doing a little more digging, I came across this description, showing how it is faster for computing in a variety of arithmetic situations: http://scipy.github.io/old-wiki/pages/EricsBroadcastingDoc \n",
    " below I worked a short snippet of code to demonstrate a little how broadcasting is faster then using some diagonalized weight matrix. \n",
    "\n",
    " - The initial comment section for each function was particularly helpful. It laid out what exactly was going to happen and I thought the commenting in this section was great.\n",
    "\n",
    "\n",
    "__Cons__\n",
    " - In the propogation function. It wasn't clear to me why the bias variable was being used or why the cost was being divided by m, the number of training examples. I understand this is something done in practice by many, but establishing a motivation or reasoning behind it could be added and certainly wouldn't take much time at all.\n",
    " \n",
    " - The biggest thing for me was the lack of context for solving these problems. That said, knowing the background I certainly learned a few things by going through the code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcast vs. Diagonals and matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "\n",
    "def dot_test( W, A ):\n",
    "    return np.dot( (W**0.5) , A )\n",
    "\n",
    "def broad_test( W, A ):\n",
    "    N=W.shape[0]\n",
    "    return (W**.5).reshape(W.shape[0],1) * A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N  = 1000\n",
    "A  = np.random.random([ N, N ])\n",
    "W1 = np.eye( N ) \n",
    "W2 = np.ones( N )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 2.27 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "broad_test( W2, A )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 25.3 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dot_test( W1, A )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *_Exercise 2: Peer Review_*\n",
    "\n",
    "The in class illustration used was an excellent demonstration of the different types of gradient descent. \n",
    "\n",
    "Exercise 2 was particularly well put together, there was a lot a learned from it and I will highlight a few things. \n",
    "\n",
    "__Pro__\n",
    "\n",
    " - The minibatches set-up is set-up was particularly good. Before I had been using a single learning example version, knowing the batch would be added in later. This setup allows both types of descent to be incorporated. The random seed feature was also something I hadn't quite thought through yet.\n",
    " - One aspect that took some time struggling with to figure out was the infs that would sometimes show-up when performing the log-likelihood cost during my gradient descent algorithms. The masking procedure used or the \"check for overflow\" portion of your code (in the avg_log-likelihood function) is something I didn't think of but will use in my own code for sure. \n",
    "\n",
    "__Con__\n",
    " - One drawback that I think is highlighted when going through the code, without the presentation to go along with it is the lack of description or motivation or contextualizing each of the code modules in the jupyter notebook. During class, each of these aspects could be discussed. But I think short descriptions for each part could go a long way to making the exercise deomonstration look quite professional and publishable online as an example for people looking for this kind of thing on the internet.\n",
    " - My second drawback is related to my first often time the code got a little messy with the inline comments. This may be partially due to my amatuer background coding in python. But, some clean-up with commented out lines and some markdown comments between code snippets could go a long way I think it making this good code better.\n",
    " - When comparing the batch gradient and stochastic gradient plot, I think it would have been better to demonstrate with the same learning rate. This would really get the point across. \n",
    " - A convergence check could also be used to demonstrate that often times stochastic gradient descent can get to a solution much quicker than regular descent. \n",
    " - Expanding with text on what's happening in the code could go a long way to tell a good story about how these different functions are working (much like the first figure in the notebook described). \n",
    " \n",
    "Overall, I think the strongest part of the exercise 2 is the set-up with the randomization and the batch function, the ability for the stochastic functions to work on a few different datasets (which isn't always a trivial thing), and the plotting was great (I don't know why I didn't think about writing a plotting function!). A few things to improve on would be the convergence check, the explanation between code snippets what contextualize what exactly is happening (maybe a moot point). I think with a little explanation before and after code could go a long way to making this short exercise a super informative piece that many people could read and access to learn about stochastic gradient descent! \n",
    " \n",
    " \n",
    "### *__Conclusion__*\n",
    "\n",
    "In conclusion, I learned a lot of python specific techniques that will certainly help me develop better, more reproducible and cleaning code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
